import time
from typing import Union, List, Dict
import openai


import os
from openai import AzureOpenAI
from azure.identity import AzureCliCredential, ChainedTokenCredential, DefaultAzureCredential, get_bearer_token_provider
import json
import requests
import os
import time
import atexit
import subprocess
import random

# pip install openai==1.35.13
def get_gpt_output(message: Union[str, List[Dict[str, str]]], 
                    model: str = "gpt-4o", 
                    max_new_tokens: int = 2048, 
                    temperature: float = 1.0, 
                    max_retry: int = 1,
                    sleep_time: int = 60,
                    json_object: bool = False,
                    **kwargs) -> str:
    """
    Call the OpenAI API to get the GPT model output for a given prompt.

    Args:
        message (Union[str, List[Dict[str, str]]]): The input message or a list of message dicts.
        model (str): The model to use for completion. Default is "gpt-4o-1106-preview".
        max_new_tokens (int): Maximum number of tokens to generate. Default is 2048.
        temperature (float): Sampling temperature. Default is 1.0.
        max_retry (int): Maximum number of retries in case of an error. Default is 1.
        sleep_time (int): Sleep time between retries in seconds. Default is 60.
        json_object (bool): Whether to output in JSON format. Default is False.

    Returns:
        str: The completed text generated by the GPT model.

    Raises:
        Exception: If the completion fails after the maximum number of retries.
    """
        
    if json_object:
        if isinstance(message, str) and 'json' not in message.lower():
            message = 'You are a helpful assistant designed to output JSON. ' + message
        
    if isinstance(message, str):
        messages = [{"role": "user", "content": message}]
    else:
        messages = message

    if json_object:
        kwargs.update({"response_format": {"type": "json_object"}}) 

    client = openai.OpenAI()

    for cnt in range(max_retry):
        try:
            chat = client.chat.completions.create(
                messages=messages,
                model=model,
                temperature=temperature,
                max_tokens=max_new_tokens,
                **kwargs
            )
            if chat.choices[0].message.content is None:
                print('Warning! None response! Could be due to safety filter.\n', chat)
                return ' '
            if json_object:
                return json.loads(chat.choices[0].message.content)
            else:
                return chat.choices[0].message.content
        except Exception as e:
            print(f"Attempt {cnt} failed: {e}. Retrying after {sleep_time} seconds...")
            time.sleep(sleep_time)
    
    raise Exception("Failed to get GPT output after maximum retries")
    

# get environment variable
# substrate = os.environ.get("USE_SUB") == "true" # substrate
# self_hosted = os.environ.get("USE_SNAP") == "true" # use openai key
# use_gcr = os.environ.get("USE_GCR") == "true" # use personal gcr


# assert not (substrate and self_hosted), "Both substrate and self-hosted cannot be True at the same time."
# if not (substrate or self_hosted or use_gcr):
#     def get_gpt_output(message: Union[str, List[Dict[str, str]]], 
#                     model: str = "gpt-4o", 
#                     max_new_tokens: int = 2048, 
#                     temperature: float = 1.0, 
#                     max_retry: int = 1,
#                     sleep_time: int = 60,
#                     json_object: bool = False,
#                     **generation_kwargs) -> str:
#         """
#         Call the OpenAI API to get the GPT model output for a given prompt.

#         Args:
#             message (Union[str, List[Dict[str, str]]]): The input message or a list of message dicts.
#             model (str): The model to use for completion. Default is "gpt-4o-1106-preview".
#             max_new_tokens (int): Maximum number of tokens to generate. Default is 2048.
#             temperature (float): Sampling temperature. Default is 1.0.
#             max_retry (int): Maximum number of retries in case of an error. Default is 1.
#             sleep_time (int): Sleep time between retries in seconds. Default is 60.
#             json_object (bool): Whether to output in JSON format. Default is False.

#         Returns:
#             str: The completed text generated by the GPT model.

#         Raises:
#             Exception: If the completion fails after the maximum number of retries.
#         """
#         # randomly select one from 1, 2, 3

#         def run_with_client(message, client):
#             if json_object:
#                 if isinstance(message, str) and 'json' not in message.lower():
#                     message = 'You are a helpful assistant designed to output JSON. ' + message
#             if isinstance(message, str):
#                 messages = [{"role": "user", "content": message}]
#             else:
#                 messages = message
#             kwargs = {"response_format": {"type": "json_object"}} if json_object else {}
#             for i in range(max_retry):
#                 chat = None
#                 try:
#                     chat = client.chat.completions.create(
#                         messages=messages,
#                         model=model,
#                         temperature=temperature,
#                         max_tokens=max_new_tokens,
#                         **kwargs, **generation_kwargs
#                     )
#                     if chat.choices[0].message.content is None or not isinstance(chat.choices[0].message.content, str) or chat.choices[0].finish_reason == "content_filter":
#                         print('Warning! None response! Could be due to safety filter. Retrun " "\n', chat)
#                         return ' '
#                     if json_object:
#                         return json.loads(chat.choices[0].message.content)
#                     else:
#                         return chat.choices[0].message.content
#                 except Exception as e:
#                     if "The response was filtered due to the prompt triggering Azure OpenAI's content management policy." in str(e):
#                         print('Detected unsafe content. Returning empty string.', e)
#                         return ' '
#                     if not "Rate limit is exceeded" in str(e):
#                         print(f"Attempt {i} failed: {e}.\nchat={chat}\nRetrying after {sleep_time} seconds...")
#                     time.sleep(sleep_time)
#             raise Exception("Failed to get GPT output after maximum retries")

#         cnt = 0
#         while True:
#             try:
#                 azure_credential = ChainedTokenCredential(
#                     AzureCliCredential(),
#                     DefaultAzureCredential(
#                         exclude_cli_credential=True,
#                         exclude_environment_credential=True,
#                         exclude_shared_token_cache_credential=True,
#                         exclude_developer_cli_credential=True,
#                         exclude_powershell_credential=True,
#                         exclude_interactive_browser_credential=True,
#                         exclude_visual_studio_code_credentials=True,
#                         managed_identity_client_id=os.environ.get("DEFAULT_IDENTITY_CLIENT_ID"),
#                     )
#                 )

#                 num = random.choice([1, 3])
#                 endpoint = f"https://dl-openai-{num}.openai.azure.com/"
#                 token_provider = get_bearer_token_provider(azure_credential,
#                     "https://cognitiveservices.azure.com/.default")
#                 client = AzureOpenAI(
#                     api_version="2024-02-15-preview",
#                     azure_endpoint=endpoint,
#                     azure_ad_token_provider=token_provider
#                 )
#                 return run_with_client(message, client)
#             except Exception as e:
#                 try:
#                     process = subprocess.Popen(["az", "login", "--use-device-code"])
#                     process.wait(timeout=100)
#                 except subprocess.TimeoutExpired:
#                     process.terminate()
#                     try:
#                         process.wait(timeout=5) 
#                     except subprocess.TimeoutExpired:
#                         process.kill()
#                         process.wait(timeout=5)
#                 except Exception as e:
#                     print("An error occurred:", e)
#                 print(f"[Rank {os.environ.get('RANK')} | Attempt {cnt}] Timeout expired. Retry Token Provider.")
            
#             cnt += 1       

# if substrate:
#     from msal import PublicClientApplication, SerializableTokenCache
#     print('Using Substrate API')
#     class LLMClient:

#         _ENDPOINT = 'https://fe-26.qas.bing.net/sdf/'
#         _SCOPES = ['api://68df66a4-cad9-4bfd-872b-c6ddde00d6b2/access']
#         _API = 'chat/completions'

#         def __init__(self, endpoint=None):
#             self._cache = SerializableTokenCache()
#             atexit.register(lambda:
#                 open('.llmapi.bin', 'w').write(self._cache.serialize())
#                 if self._cache.has_state_changed else None)
            
#             self._app = PublicClientApplication('68df66a4-cad9-4bfd-872b-c6ddde00d6b2',
#                                                 authority='https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47', 
#                                                 token_cache=self._cache)
#             if endpoint != None:
#                 LLMClient._ENDPOINT = endpoint
#             LLMClient._ENDPOINT += self._API
#             if os.path.exists('.llmapi.bin'):
#                 self._cache.deserialize(open('.llmapi.bin', 'r').read())

#         def send_request(self, model_name, request):
#             # get the token
#             # token = self._get_token()
#             token = os.environ.get("SUBSTRATE_TOKEN")

#             # populate the headers
#             headers = {
#                 'Content-Type':'application/json', 
#                 'Authorization': 'Bearer ' + token, 
#                 'X-ModelType': model_name,
#                 "X-ScenarioGUID": "30297a6a-a182-4cb8-a72d-95e2fc4fd8fc"
#                 }

#             body = str.encode(json.dumps(request))
#             response = requests.post(LLMClient._ENDPOINT, data=body, headers=headers)
            
#             if response.status_code != 200:
#                 raise Exception(f"Request failed with status code {response.status_code}. Response: {response.text}")


#             return response.json()

#         def _get_token(self):
#             accounts = self._app.get_accounts()
#             result = None

#             if accounts:
#                 # Assuming the end user chose this one
#                 chosen = accounts[0]

#                 # Now let's try to find a token in cache for this account
#                 result = self._app.acquire_token_silent(LLMClient._SCOPES, account=chosen)

#             if not result:
#                 # result = self._app.acquire_token_interactive(scopes=LLMClient._SCOPES, parent_window_handle=self._app.CONSOLE_WINDOW_HANDLE)
#                 # if 'error' in result:
#                 #     raise ValueError(
#                 #         f"Failed to acquire token. Error: {json.dumps(result, indent=4)}"
#                 #     )
                
#                 flow = self._app.initiate_device_flow(scopes=LLMClient._SCOPES)
    
#                 if "user_code" not in flow:
#                     raise ValueError(
#                         "Fail to create device flow. Err: %s" % json.dumps(flow, indent=4))
    
#                 print(flow["message"])
#                 result = self._app.acquire_token_by_device_flow(flow)
#             return result["access_token"]


#     llm_client = LLMClient()
    

#     def get_gpt_output(message: Union[str, List[Dict[str, str]]], 
#                     model: str = "gpt-4o-1106-preview", 
#                     max_new_tokens: int = 2048, 
#                     temperature: float = 1.0, 
#                     max_retry: int = 1,
#                     sleep_time: int = 60,
#                     json_object: bool = False) -> str:
#         model_name_mapping = {
#             'gpt-4o': 'dev-gpt-4o-2024-05-13-chat-completions',
#             'gpt-4-turbo': 'dev-gpt-4-1106-chat-completions',
#             'gpt-4': 'dev-gpt4-32k'
#         }
#         assert model in model_name_mapping, f"Model {model} not recognized."
#         """
#         Call the OpenAI API to get the GPT model output for a given prompt.

#         Args:
#             message (Union[str, List[Dict[str, str]]]): The input message or a list of message dicts.
#             model (str): The model to use for completion. Default is "gpt-4o-1106-preview".
#             max_new_tokens (int): Maximum number of tokens to generate. Default is 2048.
#             temperature (float): Sampling temperature. Default is 1.0.
#             max_retry (int): Maximum number of retries in case of an error. Default is 1.
#             sleep_time (int): Sleep time between retries in seconds. Default is 60.
#             json_object (bool): Whether to output in JSON format. Default is False.

#         Returns:
#             str: The completed text generated by the GPT model.

#         Raises:
#             Exception: If the completion fails after the maximum number of retries.
#         """
        
#         if json_object:
#             if isinstance(message, str) and 'json' not in message.lower():
#                 message = 'You are a helpful assistant designed to output JSON. ' + message

#         if isinstance(message, str):
#             messages = [{"role": "user", "content": message}]
#         else:
#             messages = message

#         kwargs = {"response_format": {"type": "json_object"}} if json_object else {}
        
#         request_data = {
#             "messages": messages,
#             "temperature": temperature,
#             "max_new_tokens": max_new_tokens
#         }
#         request_data.update(kwargs)
#         for cnt in range(max_retry):
#             try:
#                 chat = llm_client.send_request(model_name_mapping[model], request_data)
#                 if json_object:
#                     return json.loads(chat["choices"][0]["message"]["content"])
#                 else:
#                     return chat["choices"][0]["message"]["content"]
#             except Exception as e:
#                 if "Request failed with status code 401" in str(e):
#                     print("Token expired. ")
#                     raise e
                
#                 print(f"Attempt {cnt} failed: {e}. Retrying after {sleep_time} seconds...")
#                 time.sleep(sleep_time)

#         raise Exception("Failed to get GPT output after maximum retries")

# elif self_hosted:
#     print('Using SELF API')
#     # pip install openai==1.35.13
#     def get_gpt_output(message: Union[str, List[Dict[str, str]]], 
#                        model: str = "gpt-4o", 
#                        max_new_tokens: int = 2048, 
#                        temperature: float = 1.0, 
#                        max_retry: int = 1,
#                        sleep_time: int = 60,
#                        json_object: bool = False,
#                        **kwargs) -> str:
#         """
#         Call the OpenAI API to get the GPT model output for a given prompt.

#         Args:
#             message (Union[str, List[Dict[str, str]]]): The input message or a list of message dicts.
#             model (str): The model to use for completion. Default is "gpt-4o-1106-preview".
#             max_new_tokens (int): Maximum number of tokens to generate. Default is 2048.
#             temperature (float): Sampling temperature. Default is 1.0.
#             max_retry (int): Maximum number of retries in case of an error. Default is 1.
#             sleep_time (int): Sleep time between retries in seconds. Default is 60.
#             json_object (bool): Whether to output in JSON format. Default is False.

#         Returns:
#             str: The completed text generated by the GPT model.

#         Raises:
#             Exception: If the completion fails after the maximum number of retries.
#         """
            
#         if json_object:
#             if isinstance(message, str) and 'json' not in message.lower():
#                 message = 'You are a helpful assistant designed to output JSON. ' + message
            
#         if isinstance(message, str):
#             messages = [{"role": "user", "content": message}]
#         else:
#             messages = message

#         if json_object:
#             kwargs.update({"response_format": {"type": "json_object"}}) 

#         client = openai.OpenAI()

#         for cnt in range(max_retry):
#             try:
#                 chat = client.chat.completions.create(
#                     messages=messages,
#                     model=model,
#                     temperature=temperature,
#                     max_tokens=max_new_tokens,
#                     **kwargs
#                 )
#                 if chat.choices[0].message.content is None:
#                     print('Warning! None response! Could be due to safety filter.\n', chat)
#                     return ' '
#                 if json_object:
#                     return json.loads(chat.choices[0].message.content)
#                 else:
#                     return chat.choices[0].message.content
#             except Exception as e:
#                 print(f"Attempt {cnt} failed: {e}. Retrying after {sleep_time} seconds...")
#                 time.sleep(sleep_time)
        
#         raise Exception("Failed to get GPT output after maximum retries")

# elif use_gcr:
#     print('Using GCR API')
#     # pip install openai==0.28.0
#     def get_gpt_output(message: Union[str, List[Dict[str, str]]], 
#                         model: str = "gpt-4o-1106-preview", 
#                         max_new_tokens: int = 2048, 
#                         temperature: float = 1.0, 
#                         max_retry: int = 1,
#                         sleep_time: int = 60,
#                         json_object: bool = False) -> str:
#         """
#         Call the OpenAI API to get the GPT model output for a given prompt.

#         Args:
#             message (Union[str, List[Dict[str, str]]]): The input message or a list of message dicts.
#             model (str): The model to use for completion. Default is "gpt-4o-1106-preview".
#             max_new_tokens (int): Maximum number of tokens to generate. Default is 2048.
#             temperature (float): Sampling temperature. Default is 1.0.
#             max_retry (int): Maximum number of retries in case of an error. Default is 1.
#             sleep_time (int): Sleep time between retries in seconds. Default is 60.
#             json_object (bool): Whether to output in JSON format. Default is False.

#         Returns:
#             str: The completed text generated by the GPT model.

#         Raises:
#             Exception: If the completion fails after the maximum number of retries.
#         """
#         if json_object:
#             if isinstance(message, str) and 'json' not in message.lower():
#                 message = 'You are a helpful assistant designed to output JSON. ' + message

#         if isinstance(message, str):
#             messages = [{"role": "user", "content": message}]
#         else:
#             messages = message

#         kwargs = {"response_format": {"type": "json_object"}} if json_object else {}

#         for cnt in range(max_retry):
#             try:
#                 chat = openai.ChatCompletion.create(
#                     messages=messages,
#                     deployment_id=model,
#                     model=model,
#                     temperature=temperature,
#                     max_tokens=max_new_tokens,
#                     **kwargs
#                 )
#                 if chat.choices[0].message.content is None or not isinstance(chat.choices[0].message.content, str):
#                     print('Warning! None response! Could be due to safety filter.\n', chat)
#                     return ' '
#                 if json_object:
#                     return json.loads(chat.choices[0].message.content)
#                 else:
#                     return chat.choices[0].message.content
#             except Exception as e:
#                 print(f"Attempt {cnt} failed: {e}. Retrying after {sleep_time} seconds...")
#                 time.sleep(sleep_time)

#         raise Exception("Failed to get GPT output after maximum retries")
